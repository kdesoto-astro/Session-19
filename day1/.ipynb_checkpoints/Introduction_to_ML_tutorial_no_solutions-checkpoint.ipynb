{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dba511",
   "metadata": {},
   "source": [
    "# Tutorial for Introduction to ML Lecture\n",
    "\n",
    "version 0.1, September 2023\n",
    "\n",
    "Bryan Scott, CIERA/Northwestern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c6b0f",
   "metadata": {},
   "source": [
    "## Problem 1: Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07842041",
   "metadata": {},
   "source": [
    "A good starting point for Machine Learning is the Bayes classifier. The basic idea is to assign the most probable label to each data point using Bayes theorem, we take:\n",
    "\n",
    "$$\n",
    "p(y | x_n) \\propto p(y)p(x_i, ..., x_n | y)\n",
    "$$\n",
    "\n",
    "where y is a label for a data point and the $x_n$ are the features of the data that we want to use to classify each data point. A $\\textit{Naive} Bayes$ classifier makes an important simplifying assumptions that gives it the name - it assumes that the conditional probabilities are independent, $p(x_i, ..., x_n | y) = p(x_i|y)... p(x_n | y)$. That is, the probability of observing any individual feature doesn't depend on any of the other features. Our task is to construct this classifier from a set of examples we've observed previously and compare it to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084db8",
   "metadata": {},
   "source": [
    "### Part 0: Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e543f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsst_data[0:1000].to_csv('session_19_DC2_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c682ccc",
   "metadata": {},
   "source": [
    "### Loading and splitting the data. \n",
    "\n",
    "Read in the data, then start by selecting the id, fluxes, and object truth type in the lsst data file you've been provided. \n",
    "\n",
    "Once you have selected those, randomly split the data into two arrays, one containing 80% of the data, and a second array containing 20% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94094b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] [1 2]\n"
     ]
    }
   ],
   "source": [
    "lsst_data = pd.read_csv('simulated_extragalactic_data.csv') #path to your data\n",
    "\n",
    "lsst_data_to_classify = lsst_data[\n",
    "    [\n",
    "        'id',\n",
    "        'flux_g',\n",
    "        'flux_i',\n",
    "        'flux_r',\n",
    "        'flux_u',\n",
    "        'flux_y',\n",
    "        'flux_z',\n",
    "        'truth_type',\n",
    "    ]\n",
    "]\n",
    "lsst_data_to_classify = lsst_data_to_classify[lsst_data_to_classify['truth_type'] < 3]\n",
    "# change to log fluxes\n",
    "lsst_data_to_classify[[\n",
    "    'flux_g',\n",
    "    'flux_i',\n",
    "    'flux_r',\n",
    "    'flux_u',\n",
    "    'flux_y',\n",
    "    'flux_z',\n",
    "]] = np.log10(lsst_data_to_classify[[\n",
    "    'flux_g',\n",
    "    'flux_i',\n",
    "    'flux_r',\n",
    "    'flux_u',\n",
    "    'flux_y',\n",
    "    'flux_z',\n",
    "]])\n",
    "\n",
    "N = len(lsst_data_to_classify)\n",
    "truth_types = lsst_data_to_classify['truth_type'].to_numpy()\n",
    "\n",
    "#stratified split\n",
    "train_idxs = np.array([])\n",
    "test_idxs = np.array([])\n",
    "\n",
    "for tt in np.unique(truth_types):\n",
    "    tt_idx = np.where(truth_types == tt)[0]\n",
    "    train_idxs_i = np.random.choice(\n",
    "            tt_idx,\n",
    "            size=round(0.8*len(tt_idx)),\n",
    "            replace=False\n",
    "        )\n",
    "    train_idxs = np.append(train_idxs, train_idxs_i)\n",
    "    test_idxs = np.append(test_idxs, np.setdiff1d(tt_idx, train_idxs_i))\n",
    "\n",
    "\n",
    "train_data = lsst_data_to_classify.iloc[train_idxs]\n",
    "test_data = lsst_data_to_classify.iloc[test_idxs]\n",
    "\n",
    "print(np.unique(train_data['truth_type']), np.unique(test_data['truth_type']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2de21",
   "metadata": {},
   "source": [
    "### Part 1: Estimate Class Frequency in the training set\n",
    "\n",
    "One of the ingredients in our classifier is p(y), the unconditional class probabilities. \n",
    "\n",
    "We can get this by counting the number of rows belonging to each class in train_data and dividing by the length of the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0186b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8285356695869838 0.17146433041301626\n",
      "(0.8291457286432161, 0.1708542713567839)\n"
     ]
    }
   ],
   "source": [
    "def estimate_class_probabilities(x_train):\n",
    "    \"\"\"\n",
    "    Computes unconditional class probabilities. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        ints p1, p2: unconditional probability of an element of the training set belonging to class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = len(x_train[x_train['truth_type'] == 1]) / len(x_train)\n",
    "    p2 = len(x_train[x_train['truth_type'] == 2]) / len(x_train)\n",
    "    return p1, p2\n",
    "\n",
    "p1, p2 = estimate_class_probabilities(train_data)\n",
    "print(p1, p2)\n",
    "print(estimate_class_probabilities(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aa268",
   "metadata": {},
   "source": [
    "### Part 2:  Feature Likelihoods\n",
    "\n",
    "We are assuming that the relationship between the classes and feature probabilities are related via:\n",
    "\n",
    "$p(x_i, ..., x_n | y) =  p(x_i|y)... p(x_n | y)$\n",
    "\n",
    "however, we still need to make an assumption about the functional form of the $p(x_i | y)$. As a simple case, we will assume $p(x_i | y)$ follows a Gaussian distribution given by:\n",
    "\n",
    "$$\n",
    "p(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y}} \\exp{\\left(-\\frac{(x_i - \\mu)^2}{\\sigma_y^2}\\right)}\n",
    "$$\n",
    "\n",
    "and we will make a maximum likelihood estimate of $\\mu$ and $\\sigma_y$ from the data. This means using empirical estimates $\\bar{x}$ and $\\hat{\\sigma}$ as estimators of the true parameters $\\mu$ and $\\sigma_y$. \n",
    "\n",
    "Write a fitting function that takes the log of the fluxes and returns an estimate of the parameters of the per-feature likelihoods for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609b65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_feature_likelihood_parameters(x_train, label):\n",
    "    \"\"\"\"\n",
    "    Computes MAP estimates for the class conditional likelihood. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    "        label (int): training labels for the classifier \n",
    " \n",
    "    Returns:\n",
    "        means, stdevs (array): MAP estimates of the Gaussian conditional probability distributions for a specific class\n",
    "    \"\"\"\n",
    "    fluxes = x_train[x_train['truth_type'] == label][[\n",
    "        'flux_g',\n",
    "        'flux_i',\n",
    "        'flux_r',\n",
    "        'flux_u',\n",
    "        'flux_y',\n",
    "        'flux_z',\n",
    "    ]].to_numpy()\n",
    "    means = np.mean(fluxes)\n",
    "    stddevs = np.std(fluxes)\n",
    "        \n",
    "    return means, stddevs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbeb61",
   "metadata": {},
   "source": [
    "### Part 3: MAP Estimates of the Class Probabilities\n",
    "\n",
    "Now that we have the unconditional class probabilities and the parameters of the per feature likelihoods in hand, we can put this all together to build the classifier. Use the methods you have already written to write a function that takes in the training data and returns fit parameters. Once you have done that, write a method that takes the fit parameters as an argument and predicts the class of new (and unseen) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a17f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "\n",
    "def loglikelihood(log_fluxes, mean, std):\n",
    "    \"\"\"Calculate the loglikelihood from log fluxes, means, and stddevs.\n",
    "    \"\"\"\n",
    "    constant = -0.5*np.sum(np.log(2.*np.pi*std))\n",
    "    return constant - 0.5*np.sum((log_fluxes - mean)**2 / std**2)\n",
    "\n",
    "def fit(x_train):\n",
    "    \"\"\"\"\n",
    "    Convenience function to perform fitting on the training data\n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std: see documentation for per_feature_likelihood_parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute probabilities and MAP estimates of the Gaussian distribution's parameters using the methods you wrote above\n",
    "    p1, p2 = estimate_class_probabilities(x_train)\n",
    "    class_1_mean, class_1_std = per_feature_likelihood_parameters(x_train, 1)\n",
    "    class_2_mean, class_2_std = per_feature_likelihood_parameters(x_train, 2)\n",
    "    return p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01867cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x_test, class_probability, class_means, class_dev):\n",
    "    \"\"\"\"\n",
    "    Predict method\n",
    "     \n",
    "    Args:\n",
    "        x_test (array): data to perform classification on\n",
    "        class_probability (array): unconditional class probabilities\n",
    "        class_means, class_dev (array): MAP estimates produced by the fit method\n",
    " \n",
    "    Returns:\n",
    "        predict_List (list): class membership predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute probabilities of an element of the test set belonging to class 1 or 2\n",
    "    predict_list = []\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        log_fluxes = x_test.iloc[i][[\n",
    "            'flux_g',\n",
    "            'flux_i',\n",
    "            'flux_r',\n",
    "            'flux_u',\n",
    "            'flux_y',\n",
    "            'flux_z',\n",
    "        ]].to_numpy()\n",
    "        ll_1 = loglikelihood(log_fluxes, class_means[0], class_dev[0])\n",
    "        ll_2 = loglikelihood(log_fluxes, class_means[1], class_dev[1])\n",
    "        if np.log(class_probability[0]) + ll_1 > np.log(class_probability[1]) + ll_2:\n",
    "            predict_list.append(1)\n",
    "        else:\n",
    "            predict_list.append(2)\n",
    "    \n",
    "    return np.array(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e345409e-aa1b-4dd1-ac39-d8bc594cfb09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 2 1 1 1 1 2 2 1 1\n",
      " 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 1 1\n",
      " 1 1 2 2 1 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1\n",
      " 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 2 1 2 2 2 1 2 2 2 2 2 1 2 1\n",
      " 2 2 2 2 2 2 2 2 1 1 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# run everything\n",
    "(\n",
    "    p1,\n",
    "    p2,\n",
    "    class_1_mean,\n",
    "    class_2_mean,\n",
    "    class_1_std,\n",
    "    class_2_std\n",
    ") = fit(train_data)\n",
    "means_combined = np.array([class_1_mean, class_2_mean])\n",
    "devs_combined = np.array([class_1_std, class_2_std])\n",
    "predict_list = predict(\n",
    "    test_data,\n",
    "    np.array([p1, p2]),\n",
    "    means_combined,\n",
    "    devs_combined\n",
    ")\n",
    "print(predict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067d9e3",
   "metadata": {},
   "source": [
    "### Part 4: Metrics\n",
    "\n",
    "After creating a classifier, you now want to evaluate it in terms of how often it correctly and incorrectly classifies the objects in your training set. To do this, we'll design a confusion matrix. A confusion matrix is a matrix whose entries are the counts of the predicted vs actual class. For example, the first entry is the count of objects that are predicted to be of class 1 and actually are of class 1 and so on, while the off-diagonal elements would be instances of class 1 that are predicted to be of class 2, and instances of class 2 that are predicted to be of class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a366c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0    1   2\n",
      "row_0         \n",
      "1      142  23\n",
      "2        9  25\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(df_confusion, cmap=\"Purples\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Convenience function to plot the confusion matrix from a pd.crosstab object. Hint: use plt.matshow and choose a sensible color map.\n",
    "    \n",
    "    Args:\n",
    "        df_confusion (pd.crosstab): A pd.crosstab object.\n",
    "        \n",
    "    Returns:\n",
    "        null \n",
    "    \"\"\"\n",
    "    plt.matshow(df_confusion.to_numpy(), cmap=cmap)\n",
    "\n",
    "true_labels = test_data['truth_type'].to_numpy()\n",
    "df_confusion = pd.crosstab(\n",
    "    index=true_labels,\n",
    "    columns=predict_list,\n",
    "    #rownames=[\"1\",\"2\"],\n",
    "    #colnames=[\"1\",\"2\"]\n",
    ")\n",
    "\n",
    "print(df_confusion)\n",
    "plot_confusion_matrix(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c8763",
   "metadata": {},
   "source": [
    "## Problem 2: The Cramer-Rao bound (pen & paper, challenging, optional)\n",
    "\n",
    "As we saw in the lecture, the Cramer-Rao bound is an important result in statistics that has intuitive consequences for many applied problems in ML. The proof of the Cramer-Rao bound can be insightful to work through. \n",
    "\n",
    "The starting point for the proof of the bound is the Cauchy-Schwarz inequality, which can be used to show that:\n",
    "\n",
    "$$\n",
    "[Cov(U, V)]^2 \\le Var(U)Var(V)\n",
    "$$\n",
    "\n",
    "Starting from the definitions that U = T(X), where T(X) is an estimator of some parameter $\\theta$ of the distribution $f(X|\\theta)$ from which the data is sampled, and V = $\\frac{\\partial}{\\partial \\theta} log f(X |\\theta)$. Use the Cauchy-Schwarz inequality to show the Cramer-Rao bound for these choices of U and V. \n",
    "\n",
    "$\\textit{Hint:}$ you will need the fact that the $\\mathbb{E}(V) = 0$, where $\\mathbb{E}$ is the expectation of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1432f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
